{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "import yaml\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "import argparse\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:6' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Animals Dataset class \n",
    "class AnimalsDataset(Dataset):\n",
    "    r\"\"\"\n",
    "    Nothing special here. Just a simple dataset class for mnist images.\n",
    "    Created a dataset class rather using torchvision to allow\n",
    "    replacement with any other image dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, split, im_path, im_ext='jpg'):\n",
    "        r\"\"\"\n",
    "        Init method for initializing the dataset properties\n",
    "        :param split: train/test to locate the image files\n",
    "        :param im_path: root folder of images\n",
    "        :param im_ext: image extension. assumes all\n",
    "        images would be this type.\n",
    "        \"\"\"\n",
    "        self.split = split\n",
    "        self.im_ext = im_ext\n",
    "        self.images, self.labels = self.load_images(im_path)\n",
    "    \n",
    "    def load_images(self, im_path):\n",
    "        r\"\"\"\n",
    "        Gets all images from the path specified\n",
    "        and stacks them all up\n",
    "        :param im_path:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert os.path.exists(im_path), \"images path {} does not exist\".format(im_path)\n",
    "        ims = []\n",
    "        labels = []\n",
    "        for d_name in tqdm(os.listdir(im_path)):\n",
    "            for fname in glob.glob(os.path.join(im_path, d_name, '*.{}'.format(self.im_ext))):\n",
    "                ims.append(fname)\n",
    "                labels.append(d_name)\n",
    "        print('Found {} images for split {}'.format(len(ims), self.split))\n",
    "        return ims, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def get_transforms(self):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((64, 64)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        return transform\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        im = Image.open(self.images[index])\n",
    "        transform = self.get_transforms()\n",
    "        im_tensor =transform(im)\n",
    "        \n",
    "        # Convert input to -1 to 1 range.\n",
    "        im_tensor = (2 * im_tensor) - 1\n",
    "        return im_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class to the Image at any timestep\n",
    "class LinearNoiseScheduler:\n",
    "    r\"\"\"\n",
    "    Class for the linear noise scheduler that is used in DDPM.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_timesteps, beta_start, beta_end):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        \n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alpha_cum_prod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod)\n",
    "        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - self.alpha_cum_prod)\n",
    "        \n",
    "    def add_noise(self, original, noise, t):\n",
    "        r\"\"\"\n",
    "        Forward method for diffusion\n",
    "        :param original: Image on which noise is to be applied\n",
    "        :param noise: Random Noise Tensor (from normal dist)\n",
    "        :param t: timestep of the forward process of shape -> (B,)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        original_shape = original.shape\n",
    "        batch_size = original_shape[0]\n",
    "        \n",
    "        sqrt_alpha_cum_prod = self.sqrt_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n",
    "        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n",
    "        \n",
    "        # Reshape till (B,) becomes (B,1,1,1) if image is (B,C,H,W)\n",
    "        for _ in range(len(original_shape) - 1):\n",
    "            sqrt_alpha_cum_prod = sqrt_alpha_cum_prod.unsqueeze(-1)\n",
    "        for _ in range(len(original_shape) - 1):\n",
    "            sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n",
    "        \n",
    "        # Apply and Return Forward process equation\n",
    "        return (sqrt_alpha_cum_prod.to(original.device) * original\n",
    "                + sqrt_one_minus_alpha_cum_prod.to(original.device) * noise)\n",
    "        \n",
    "    def sample_prev_timestep(self, xt, noise_pred, t):\n",
    "        r\"\"\"\n",
    "            Use the noise prediction by model to get\n",
    "            xt-1 using xt and the noise predicted\n",
    "        :param xt: current timestep sample\n",
    "        :param noise_pred: model noise prediction\n",
    "        :param t: current timestep we are at\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x0 = ((xt - (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t] * noise_pred)) /\n",
    "              torch.sqrt(self.alpha_cum_prod.to(xt.device)[t]))\n",
    "        x0 = torch.clamp(x0, -1., 1.)\n",
    "        \n",
    "        mean = xt - ((self.betas.to(xt.device)[t]) * noise_pred) / (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t])\n",
    "        mean = mean / torch.sqrt(self.alphas.to(xt.device)[t])\n",
    "        \n",
    "        if t == 0:\n",
    "            return mean, x0\n",
    "        else:\n",
    "            variance = (1 - self.alpha_cum_prod.to(xt.device)[t - 1]) / (1.0 - self.alpha_cum_prod.to(xt.device)[t])\n",
    "            variance = variance * self.betas.to(xt.device)[t]\n",
    "            sigma = variance ** 0.5\n",
    "            z = torch.randn(xt.shape).to(xt.device)\n",
    "            \n",
    "            # OR\n",
    "            # variance = self.betas[t]\n",
    "            # sigma = variance ** 0.5\n",
    "            # z = torch.randn(xt.shape).to(xt.device)\n",
    "            return mean + sigma * z, x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_embedding(time_steps, temb_dim):\n",
    "    r\"\"\"\n",
    "    Convert time steps tensor into an embedding using the\n",
    "    sinusoidal time embedding formula\n",
    "    :param time_steps: 1D tensor of length batch size\n",
    "    :param temb_dim: Dimension of the embedding\n",
    "    :return: BxD embedding representation of B time steps\n",
    "    \"\"\"\n",
    "    assert temb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n",
    "    \n",
    "    # factor = 10000^(2i/d_model)\n",
    "    factor = 10000 ** ((torch.arange(\n",
    "        start=0, end=temb_dim // 2, dtype=torch.float32, device=time_steps.device) / (temb_dim // 2))\n",
    "    )\n",
    "    \n",
    "    # pos / factor\n",
    "    # timesteps B -> B, 1 -> B, temb_dim\n",
    "    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n",
    "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n",
    "    return t_emb\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Down conv block with attention.\n",
    "    Sequence of following block\n",
    "    1. Resnet block with time embedding\n",
    "    2. Attention block\n",
    "    3. Downsample using 2x2 average pooling\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim,\n",
    "                 down_sample=True, num_heads=4, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.down_sample = down_sample\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.t_emb_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(t_emb_dim, out_channels)\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [nn.GroupNorm(8, out_channels)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        self.attentions = nn.ModuleList(\n",
    "            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.down_sample_conv = nn.Conv2d(out_channels, out_channels,\n",
    "                                          4, 2, 1) if self.down_sample else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, t_emb):\n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            # Resnet block of Unet\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "            \n",
    "            # Attention block of Unet\n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "            \n",
    "        out = self.down_sample_conv(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MidBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Mid conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Resnet block with time embedding\n",
    "    2. Attention block\n",
    "    3. Resnet block with time embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads=4, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers+1)\n",
    "            ]\n",
    "        )\n",
    "        self.t_emb_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(t_emb_dim, out_channels)\n",
    "            )\n",
    "            for _ in range(num_layers + 1)\n",
    "        ])\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers+1)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [nn.GroupNorm(8, out_channels)\n",
    "                for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        self.attentions = nn.ModuleList(\n",
    "            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                for _ in range(num_layers)]\n",
    "        )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers+1)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t_emb):\n",
    "        out = x\n",
    "        \n",
    "        # First resnet block\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first[0](out)\n",
    "        out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n",
    "        out = self.resnet_conv_second[0](out)\n",
    "        out = out + self.residual_input_conv[0](resnet_input)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            # Attention Block\n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "            \n",
    "            # Resnet Block\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i+1](out)\n",
    "            out = out + self.t_emb_layers[i+1](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i+1](out)\n",
    "            out = out + self.residual_input_conv[i+1](resnet_input)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Up conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Upsample\n",
    "    1. Concatenate Down block output\n",
    "    2. Resnet block with time embedding\n",
    "    3. Attention Block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample=True, num_heads=4, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.up_sample = up_sample\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.t_emb_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(t_emb_dim, out_channels)\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [\n",
    "                nn.GroupNorm(8, out_channels)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.attentions = nn.ModuleList(\n",
    "            [\n",
    "                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.up_sample_conv = nn.ConvTranspose2d(in_channels // 2, in_channels // 2,\n",
    "                                                 4, 2, 1) \\\n",
    "            if self.up_sample else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, out_down, t_emb):\n",
    "        x = self.up_sample_conv(x)\n",
    "        x = torch.cat([x, out_down], dim=1)\n",
    "        \n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "            \n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Unet Model\n",
    "class Unet(nn.Module):\n",
    "    r\"\"\"\n",
    "    Unet model comprising\n",
    "    Down blocks, Midblocks and Uplocks\n",
    "    \"\"\"\n",
    "    def __init__(self, model_config):\n",
    "        super().__init__()\n",
    "        im_channels = model_config['im_channels']\n",
    "        self.down_channels = model_config['down_channels']\n",
    "        self.mid_channels = model_config['mid_channels']\n",
    "        self.t_emb_dim = model_config['time_emb_dim']\n",
    "        self.down_sample = model_config['down_sample']\n",
    "        self.num_down_layers = model_config['num_down_layers']\n",
    "        self.num_mid_layers = model_config['num_mid_layers']\n",
    "        self.num_up_layers = model_config['num_up_layers']\n",
    "        \n",
    "        assert self.mid_channels[0] == self.down_channels[-1]\n",
    "        assert self.mid_channels[-1] == self.down_channels[-2]\n",
    "        assert len(self.down_sample) == len(self.down_channels) - 1\n",
    "        \n",
    "        # Initial projection from sinusoidal time embedding\n",
    "        self.t_proj = nn.Sequential(\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n",
    "        )\n",
    "\n",
    "        self.up_sample = list(reversed(self.down_sample))\n",
    "        self.conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size=3, padding=(1, 1))\n",
    "        \n",
    "        self.downs = nn.ModuleList([])\n",
    "        for i in range(len(self.down_channels)-1):\n",
    "            self.downs.append(DownBlock(self.down_channels[i], self.down_channels[i+1], self.t_emb_dim,\n",
    "                                        down_sample=self.down_sample[i], num_layers=self.num_down_layers))\n",
    "        \n",
    "        self.mids = nn.ModuleList([])\n",
    "        for i in range(len(self.mid_channels)-1):\n",
    "            self.mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i+1], self.t_emb_dim,\n",
    "                                      num_layers=self.num_mid_layers))\n",
    "        \n",
    "        self.ups = nn.ModuleList([])\n",
    "        for i in reversed(range(len(self.down_channels)-1)):\n",
    "            self.ups.append(UpBlock(self.down_channels[i] * 2, self.down_channels[i-1] if i != 0 else 16,\n",
    "                                    self.t_emb_dim, up_sample=self.down_sample[i], num_layers=self.num_up_layers))\n",
    "        \n",
    "        self.norm_out = nn.GroupNorm(8, 16)\n",
    "        self.conv_out = nn.Conv2d(16, im_channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        # Shapes assuming downblocks are [C1, C2, C3, C4]\n",
    "        # Shapes assuming midblocks are [C4, C4, C3]\n",
    "        # Shapes assuming downsamples are [True, True, False]\n",
    "        # B x C x H x W\n",
    "        out = self.conv_in(x)\n",
    "        # B x C1 x H x W\n",
    "        \n",
    "        # t_emb -> B x t_emb_dim\n",
    "        t_emb = get_time_embedding(torch.as_tensor(t).long(), self.t_emb_dim)\n",
    "        t_emb = self.t_proj(t_emb)\n",
    "        \n",
    "        down_outs = []\n",
    "        \n",
    "        for idx, down in enumerate(self.downs):\n",
    "            down_outs.append(out)\n",
    "            out = down(out, t_emb)\n",
    "        # down_outs  [B x C1 x H x W, B x C2 x H/2 x W/2, B x C3 x H/4 x W/4]\n",
    "        # out B x C4 x H/4 x W/4\n",
    "            \n",
    "        for mid in self.mids:\n",
    "            out = mid(out, t_emb)\n",
    "        # out B x C3 x H/4 x W/4\n",
    "        \n",
    "        for up in self.ups:\n",
    "            down_out = down_outs.pop()\n",
    "            out = up(out, down_out, t_emb)\n",
    "            # out [B x C2 x H/4 x W/4, B x C1 x H/2 x W/2, B x 16 x H x W]\n",
    "        out = self.norm_out(out)\n",
    "        out = nn.SiLU()(out)\n",
    "        out = self.conv_out(out)\n",
    "        # out B x C x H x W\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Function\n",
    "def train(args):\n",
    "    # Read the config file #\n",
    "    with open(args.config_path, 'r') as file:\n",
    "        try:\n",
    "            config = yaml.safe_load(file)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    print(config)\n",
    "    ########################\n",
    "    \n",
    "    diffusion_config = config['diffusion_params']\n",
    "    dataset_config = config['dataset_params']\n",
    "    model_config = config['model_params']\n",
    "    train_config = config['train_params']\n",
    "    \n",
    "    # Create the noise scheduler\n",
    "    scheduler = LinearNoiseScheduler(num_timesteps=diffusion_config['num_timesteps'],\n",
    "                                     beta_start=diffusion_config['beta_start'],\n",
    "                                     beta_end=diffusion_config['beta_end'])\n",
    "    \n",
    "    # Create the dataset\n",
    "    mnist = AnimalsDataset('train', im_path=dataset_config['im_path'])\n",
    "    mnist_loader = DataLoader(mnist, batch_size=train_config['batch_size'], shuffle=True, num_workers=4)\n",
    "    \n",
    "    # Instantiate the model\n",
    "    model = Unet(model_config).to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # Create output directories\n",
    "    if not os.path.exists(train_config['task_name']):\n",
    "        os.mkdir(train_config['task_name'])\n",
    "    \n",
    "    # Load checkpoint if found\n",
    "    if os.path.exists(os.path.join(train_config['ckpt_name'])):\n",
    "        print('Loading checkpoint as found one')\n",
    "        model.load_state_dict(torch.load(os.path.join(train_config['ckpt_name']), map_location=device))\n",
    "\n",
    "    # Specify training parameters\n",
    "    num_epochs = train_config['num_epochs']\n",
    "    optimizer = Adam(model.parameters(), lr=train_config['lr'])\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    # Run training\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        losses = []\n",
    "        for im in tqdm(mnist_loader):\n",
    "            optimizer.zero_grad()\n",
    "            im = im.float().to(device)\n",
    "            print(f\"Dimentions of image is {im.shape}\")\n",
    "            \n",
    "            # Sample random noise\n",
    "            noise = torch.randn_like(im).to(device)\n",
    "            \n",
    "            # Sample timestep\n",
    "            t = torch.randint(0, diffusion_config['num_timesteps'], (im.shape[0],)).to(device)\n",
    "            \n",
    "            # Add noise to images according to timestep\n",
    "            noisy_im = scheduler.add_noise(im, noise, t)\n",
    "            noise_pred = model(noisy_im, t)\n",
    "\n",
    "            loss = criterion(noise_pred, noise)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('Finished epoch:{} | Loss : {:.4f}'.format(\n",
    "            epoch_idx + 1,\n",
    "            np.mean(losses),\n",
    "        ))\n",
    "        torch.save(model.state_dict(), os.path.join(train_config['ckpt_name']))\n",
    "    \n",
    "    print('Done Training ...')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Arguments for ddpm training')\n",
    "parser.add_argument('--config', dest='config_path',\n",
    "                    default='/data/home/saisuchithm/godwin/DLCV/default.yaml', type=str)\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(args=[]) \n",
    "# Read the config file #\n",
    "with open(args.config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_params': {'im_path': '/data/home/saisuchithm/godwin/DLCV/animal-faces/afhq/train'}, 'diffusion_params': {'num_timesteps': 1000, 'beta_start': 0.0001, 'beta_end': 0.02}, 'model_params': {'im_channels': 3, 'im_size': 64, 'down_channels': [32, 64, 128, 256], 'mid_channels': [256, 256, 128], 'down_sample': [True, True, False], 'time_emb_dim': 128, 'num_down_layers': 1, 'num_mid_layers': 1, 'num_up_layers': 1, 'num_heads': 2}, 'train_params': {'task_name': 'default', 'batch_size': 4, 'num_epochs': 1, 'num_samples': 10, 'num_grid_rows': 10, 'lr': 0.0001, 'ckpt_name': 'ddpm_ckpt.pth'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 58.37it/s]\n",
      "/tmp/ipykernel_568068/781783397.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(train_config['ckpt_name']), map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14630 images for split train\n",
      "Loading checkpoint as found one\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3658 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimentions of image is torch.Size([4, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3658 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch:1 | Loss : 0.0275\n",
      "Done Training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_759719/975590269.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(config['train_params'][\"ckpt_name\"], map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = config['model_params']\n",
    "model = Unet(model_config).to(device)\n",
    "model.load_state_dict(torch.load(config['train_params'][\"ckpt_name\"], map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, scheduler, train_config, model_config, diffusion_config):\n",
    "    r\"\"\"\n",
    "    Sample stepwise by going backward one timestep at a time.\n",
    "    Only save the final x0 prediction.\n",
    "    \"\"\"\n",
    "    xt = torch.randn((train_config['num_samples'],\n",
    "                      model_config['im_channels'],\n",
    "                      model_config['im_size'],\n",
    "                      model_config['im_size'])).to(device)\n",
    "    for i in tqdm(reversed(range(diffusion_config['num_timesteps']))):\n",
    "        # Get prediction of noise\n",
    "        noise_pred = model(xt, torch.as_tensor(i).unsqueeze(0).to(device))\n",
    "        \n",
    "        # Use scheduler to get x0 and xt-1\n",
    "        xt, x0_pred = scheduler.sample_prev_timestep(xt, noise_pred, torch.as_tensor(i).to(device))\n",
    "\n",
    "    # Save only the final x0\n",
    "    ims = torch.clamp(x0_pred, -1., 1.).detach().cpu()\n",
    "    ims = (ims + 1) / 2  # scale to [0, 1]\n",
    "    grid = make_grid(ims, nrow=train_config['num_grid_rows'])\n",
    "    img = transforms.ToPILImage()(grid)\n",
    "\n",
    "    save_dir = os.path.join(train_config['task_name'], 'samples')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    img.save(os.path.join(save_dir, 'final_x0.png'))\n",
    "    img.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(args, model):\n",
    "    # Read the config file #\n",
    "    with open(args.config_path, 'r') as file:\n",
    "        try:\n",
    "            config = yaml.safe_load(file)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    print(config)\n",
    "    ########################\n",
    "    \n",
    "    diffusion_config = config['diffusion_params']\n",
    "    model_config = config['model_params']\n",
    "    train_config = config['train_params']\n",
    "    \n",
    "    # Create the noise scheduler\n",
    "    scheduler = LinearNoiseScheduler(num_timesteps=diffusion_config['num_timesteps'],\n",
    "                                     beta_start=diffusion_config['beta_start'],\n",
    "                                     beta_end=diffusion_config['beta_end'])\n",
    "    with torch.no_grad():\n",
    "        sample(model, scheduler, train_config, model_config, diffusion_config)\n",
    "    del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Arguments for ddpm training')\n",
    "parser.add_argument('--config', dest='config_path',\n",
    "                    default='/data/home/saisuchithm/godwin/DLCV/default.yaml', type=str)\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(args=[]) \n",
    "# Read the config file #\n",
    "with open(args.config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "diffusion_config = config['diffusion_params']\n",
    "model_config = config['model_params']\n",
    "train_config = config['train_params']\n",
    "# Load model with checkpoint\n",
    "model = Unet(model_config).to(device)\n",
    "model.load_state_dict(torch.load(train_config[\"ckpt_name\"], map_location=device))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer(args, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                  Version    \n",
      "------------------------ -----------\n",
      "aiohappyeyeballs         2.4.4      \n",
      "aiohttp                  3.10.11    \n",
      "aiosignal                1.3.1      \n",
      "asttokens                3.0.0      \n",
      "async-timeout            5.0.1      \n",
      "attrs                    25.3.0     \n",
      "backcall                 0.2.0      \n",
      "bleach                   6.1.0      \n",
      "certifi                  2025.1.31  \n",
      "charset-normalizer       3.4.1      \n",
      "clip                     1.0        \n",
      "comm                     0.2.2      \n",
      "contourpy                1.1.1      \n",
      "cycler                   0.12.1     \n",
      "datasets                 3.1.0      \n",
      "debugpy                  1.8.13     \n",
      "decorator                5.2.1      \n",
      "dill                     0.3.8      \n",
      "executing                2.2.0      \n",
      "filelock                 3.16.1     \n",
      "fonttools                4.57.0     \n",
      "frozenlist               1.5.0      \n",
      "fsspec                   2024.9.0   \n",
      "ftfy                     6.2.3      \n",
      "huggingface-hub          0.30.1     \n",
      "idna                     3.10       \n",
      "importlib-metadata       8.5.0      \n",
      "importlib-resources      6.4.5      \n",
      "ipykernel                6.29.5     \n",
      "ipython                  8.12.3     \n",
      "jedi                     0.19.2     \n",
      "jinja2                   3.1.6      \n",
      "joblib                   1.4.2      \n",
      "jupyter-client           8.6.3      \n",
      "jupyter-core             5.7.2      \n",
      "kaggle                   1.7.4.2    \n",
      "kiwisolver               1.4.7      \n",
      "lmdb                     1.6.2      \n",
      "MarkupSafe               2.1.5      \n",
      "matplotlib               3.7.5      \n",
      "matplotlib-inline        0.1.7      \n",
      "mpmath                   1.3.0      \n",
      "multidict                6.1.0      \n",
      "multiprocess             0.70.16    \n",
      "nest-asyncio             1.6.0      \n",
      "networkx                 3.1        \n",
      "numpy                    1.24.4     \n",
      "nvidia-cublas-cu12       12.1.3.1   \n",
      "nvidia-cuda-cupti-cu12   12.1.105   \n",
      "nvidia-cuda-nvrtc-cu12   12.1.105   \n",
      "nvidia-cuda-runtime-cu12 12.1.105   \n",
      "nvidia-cudnn-cu12        9.1.0.70   \n",
      "nvidia-cufft-cu12        11.0.2.54  \n",
      "nvidia-curand-cu12       10.3.2.106 \n",
      "nvidia-cusolver-cu12     11.4.5.107 \n",
      "nvidia-cusparse-cu12     12.1.0.106 \n",
      "nvidia-nccl-cu12         2.20.5     \n",
      "nvidia-nvjitlink-cu12    12.8.93    \n",
      "nvidia-nvtx-cu12         12.1.105   \n",
      "opencv-python            4.11.0.86  \n",
      "packaging                24.2       \n",
      "pandas                   2.0.3      \n",
      "parso                    0.8.4      \n",
      "pexpect                  4.9.0      \n",
      "pickleshare              0.7.5      \n",
      "pillow                   10.4.0     \n",
      "pip                      20.0.2     \n",
      "pkg-resources            0.0.0      \n",
      "platformdirs             4.3.6      \n",
      "prompt-toolkit           3.0.50     \n",
      "propcache                0.2.0      \n",
      "protobuf                 5.29.4     \n",
      "psutil                   7.0.0      \n",
      "ptyprocess               0.7.0      \n",
      "pure-eval                0.2.3      \n",
      "pyarrow                  17.0.0     \n",
      "pygments                 2.19.1     \n",
      "pyparsing                3.1.4      \n",
      "python-dateutil          2.9.0.post0\n",
      "python-slugify           8.0.4      \n",
      "pytz                     2025.2     \n",
      "PyYAML                   6.0.2      \n",
      "pyzmq                    26.3.0     \n",
      "regex                    2024.11.6  \n",
      "requests                 2.32.3     \n",
      "safetensors              0.5.3      \n",
      "scikit-learn             1.3.2      \n",
      "scipy                    1.10.1     \n",
      "seaborn                  0.13.2     \n",
      "setuptools               44.0.0     \n",
      "six                      1.17.0     \n",
      "stack-data               0.6.3      \n",
      "sympy                    1.13.3     \n",
      "text-unidecode           1.3        \n",
      "threadpoolctl            3.5.0      \n",
      "timm                     1.0.15     \n",
      "torch                    2.4.1      \n",
      "torchaudio               2.4.1      \n",
      "torchvision              0.19.1     \n",
      "tornado                  6.4.2      \n",
      "tqdm                     4.67.1     \n",
      "traitlets                5.14.3     \n",
      "triton                   3.0.0      \n",
      "typing-extensions        4.13.0     \n",
      "tzdata                   2025.2     \n",
      "urllib3                  2.2.3      \n",
      "wcwidth                  0.2.13     \n",
      "webencodings             0.5.1      \n",
      "xxhash                   3.5.0      \n",
      "yarl                     1.15.2     \n",
      "zipp                     3.20.2     \n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-gen-detect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
